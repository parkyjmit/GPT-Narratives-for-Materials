{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at allenai/scibert_scivocab_cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "#Model\n",
    "model = BertForQuestionAnswering.from_pretrained(\"allenai/scibert_scivocab_cased\")\n",
    "\n",
    "#Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"allenai/scibert_scivocab_cased\" # \"m3rg-iitd/matscibert\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '''What is Machine Learning?'''\n",
    "\n",
    "paragraph = ''' Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to progressively improve their performance on a specific task. Machine learning algorithms build a mathematical model of sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in the applications of email filtering, detection of network intruders, and computer vision, where it is infeasible to develop an algorithm of specific instructions for performing the task. Machine learning is closely related to computational statistics, which focuses on making predictions using computers. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a field of study within machine learning, and focuses on exploratory data analysis through unsupervised learning.In its application across business problems, machine learning is also referred to as predictive analytics. '''\n",
    "            \n",
    "encoding = tokenizer.encode_plus(text=question,text_pair=paragraph)\n",
    "\n",
    "inputs = encoding['input_ids']  #Token embeddings\n",
    "sentence_embedding = encoding['token_type_ids']  #Segment embeddings\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs) #input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_scores, end_scores = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'start_logits'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at allenai/scibert_scivocab_cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.00014875772467348725,\n",
       " 'start': 771,\n",
       " 'end': 793,\n",
       " 'answer': 'theory and application'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\", model=\"allenai/scibert_scivocab_cased\")\n",
    "question_answerer(question=question, context=paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet('/mnt/hdd1/LaMDa/mp_3d_2020_gpt_narratives.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['atoms', 'space group symbol', 'crystal system',\n",
       "       'energy per atom (eV/atom)', 'volume (Å³)',\n",
       "       'formation energy per atom (eV/atom)', 'pretty formula',\n",
       "       'energy above hull (eV/atom)', 'band gap (eV)', 'density (g/cm³)',\n",
       "       'total magnetization (μB/f.u.)', 'oxide type',\n",
       "       'scintillation attenuation length (cm)', 'enthalpy per atom (eV/atom)',\n",
       "       'gpt_text', 'gpt_explanation', 'reduced_formula', 'text',\n",
       "       'structure_question_list', 'composition_question_list',\n",
       "       'stable_question_list', 'is_stable', 'oxide_question_list',\n",
       "       'comp_struc_question_list'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['This material is cubic.', 'This material is tetragonal.',\n",
       "       'This material is hexagonal.', 'This material is orthorhombic.',\n",
       "       'This material is trigonal.', 'This material is monoclinic.',\n",
       "       'This material is triclinic.'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['structure_question_list'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at allenai/scibert_scivocab_cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMultipleChoice\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('/mnt/hdd1/LaMDa/mp_3d_2020_gpt_narratives.parquet')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_cased\")\n",
    "model = AutoModelForMultipleChoice.from_pretrained(\"allenai/scibert_scivocab_cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "126335it [1:28:09, 23.89it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.cuda()\n",
    "answer_list = []\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    try:\n",
    "        row['atoms'].pop('coords')\n",
    "        row['atoms'].pop('props')\n",
    "        prompt = 'Which of the following statements is most correct regarding the given material?' + str(row['atoms'])\n",
    "        inputs = tokenizer([[prompt, candidate] for candidate in row['structure_question_list']], return_tensors=\"pt\", padding=True)\n",
    "        labels = torch.tensor(0).unsqueeze(0).cuda()\n",
    "\n",
    "        outputs = model(**{k: v.unsqueeze(0).cuda() for k, v in inputs.items()}, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        answer_list.append(logits.argmax().item())\n",
    "    except:\n",
    "        answer_list.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10228361103415522"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count -1 \n",
    "sum([i == 0 for i in answer_list])/len(answer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "126335it [2:17:48, 15.28it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.cuda()\n",
    "comp_answer_list = []\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    try:\n",
    "        # row['atoms'].pop('coords')\n",
    "        # row['atoms'].pop('props')\n",
    "        prompt = 'Which of the following statements is most correct regarding the given material?' + str(row['atoms'])\n",
    "        inputs = tokenizer([[prompt, candidate] for candidate in row['composition_question_list']], return_tensors=\"pt\", padding=True)\n",
    "        labels = torch.tensor(0).unsqueeze(0).cuda()\n",
    "\n",
    "        outputs = model(**{k: v.unsqueeze(0).cuda() for k, v in inputs.items()}, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        comp_answer_list.append(logits.argmax().item())\n",
    "    except:\n",
    "        comp_answer_list.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5580638777852535, 11652)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([i == 0 for i in comp_answer_list])/len(comp_answer_list), sum([i == -1 for i in comp_answer_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "126335it [1:17:41, 27.10it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.cuda()\n",
    "oxide_answer_list = []\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    try:\n",
    "        # row['atoms'].pop('coords')\n",
    "        # row['atoms'].pop('props')\n",
    "        prompt = 'Which of the following statements is most correct regarding the given material?' + str(row['atoms'])\n",
    "        inputs = tokenizer([[prompt, candidate] for candidate in row['oxide_question_list']], return_tensors=\"pt\", padding=True)\n",
    "        labels = torch.tensor(0).unsqueeze(0).cuda()\n",
    "\n",
    "        outputs = model(**{k: v.unsqueeze(0).cuda() for k, v in inputs.items()}, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        oxide_answer_list.append(logits.argmax().item())\n",
    "    except:\n",
    "        oxide_answer_list.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4933312225432382, 7722)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([i == 0 for i in oxide_answer_list])/len(oxide_answer_list), sum([i == -1 for i in oxide_answer_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "126335it [1:30:43, 23.21it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.cuda()\n",
    "comp_struc_answer_list = []\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    try:\n",
    "        # row['atoms'].pop('coords')\n",
    "        # row['atoms'].pop('props')\n",
    "        prompt = 'Which of the following statements is most correct regarding the given material?' + str(row['atoms'])\n",
    "        inputs = tokenizer([[prompt, candidate] for candidate in row['comp_struc_question_list']], return_tensors=\"pt\", padding=True)\n",
    "        labels = torch.tensor(0).unsqueeze(0).cuda()\n",
    "\n",
    "        outputs = model(**{k: v.unsqueeze(0).cuda() for k, v in inputs.items()}, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        comp_struc_answer_list.append(logits.argmax().item())\n",
    "    except:\n",
    "        comp_struc_answer_list.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.08834448094352318, 8520)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([i == 0 for i in comp_struc_answer_list])/len(comp_struc_answer_list), sum([i == -1 for i in comp_struc_answer_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 917])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abc': array([ 5.83847,  9.16937, 16.52476]),\n",
       " 'angles': array([90., 90., 90.]),\n",
       " 'cartesian': False,\n",
       " 'elements': array(['B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B',\n",
       "        'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B',\n",
       "        'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B',\n",
       "        'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B',\n",
       "        'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B',\n",
       "        'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B',\n",
       "        'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B',\n",
       "        'B', 'B', 'B', 'B', 'B', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N',\n",
       "        'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'O', 'O', 'O', 'O'],\n",
       "       dtype=object),\n",
       " 'lattice_mat': array([array([5.8384700e+00, 0.0000000e+00, 3.5750318e-16]),\n",
       "        array([0.00000000e+00, 9.16937400e+00, 5.61462226e-16]),\n",
       "        array([ 0.      ,  0.      , 16.524762])], dtype=object)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row['atoms']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "df = pd.read_parquet('/mnt/hdd1/LaMDa/mp_3d_2020_gpt_narratives.parquet')\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    row['atoms'].pop('coords')\n",
    "    row['atoms'].pop('props')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "126335it [01:08, 1846.88it/s]\n"
     ]
    }
   ],
   "source": [
    "comp_data = {f'ending{i}': [] for i in range(len(df['composition_question_list'][0]))}\n",
    "comp_data['sent1'] = []\n",
    "comp_data['sent2'] = []\n",
    "comp_data['label'] = []\n",
    "\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    comp_data['sent1'].append('Which of the following statements is most correct regarding the given material?')\n",
    "    comp_data['sent2'].append(str(row['atoms']))\n",
    "    comp_data['label'].append(0)\n",
    "    for j in range(len(row['composition_question_list'])):\n",
    "        comp_data[f'ending{j}'].append(row['composition_question_list'][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(comp_data)\n",
    "temp_train = temp.iloc[:int(len(temp)*0.8)]\n",
    "temp_valid = temp.iloc[int(len(temp)*0.8):int(len(temp)*0.9)]\n",
    "temp_test = temp.iloc[int(len(temp)*0.9):]\n",
    "temp_train.to_json('/mnt/hdd1/LaMDa/comp_data_train.json', lines=True, orient='records')\n",
    "temp_valid.to_json('/mnt/hdd1/LaMDa/comp_data_valid.json', lines=True, orient='records')\n",
    "temp_test.to_json('/mnt/hdd1/LaMDa/comp_data_test.json', lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_data = {f'ending{i}': [] for i in range(len(df['structure_question_list'][0]))}\n",
    "str_data['sent1'] = []\n",
    "str_data['sent2'] = []\n",
    "str_data['label'] = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    str_data['sent1'].append('Which of the following statements is most correct regarding the given material?')\n",
    "    str_data['sent2'].append(str(row['atoms']))\n",
    "    str_data['label'].append(0)\n",
    "    for j in range(len(row['structure_question_list'])):\n",
    "        str_data[f'ending{j}'].append(row['structure_question_list'][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(str_data)\n",
    "temp_train = temp.iloc[:int(len(temp)*0.8)]\n",
    "temp_valid = temp.iloc[int(len(temp)*0.8):int(len(temp)*0.9)]\n",
    "temp_test = temp.iloc[int(len(temp)*0.9):]  \n",
    "temp_train.to_json('/mnt/hdd1/LaMDa/str_data_train.json', lines=True, orient='records')\n",
    "temp_valid.to_json('/mnt/hdd1/LaMDa/str_data_valid.json', lines=True, orient='records')\n",
    "temp_test.to_json('/mnt/hdd1/LaMDa/str_data_test.json', lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_str_data = {f'ending{i}': [] for i in range(len(df['comp_struc_question_list'][0]))}\n",
    "comp_str_data['sent1'] = []\n",
    "comp_str_data['sent2'] = []\n",
    "comp_str_data['label'] = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    comp_str_data['sent1'].append('Which of the following statements is most correct regarding the given material?')\n",
    "    comp_str_data['sent2'].append(str(row['atoms']))\n",
    "    comp_str_data['label'].append(0)\n",
    "    for j in range(len(row['comp_struc_question_list'])):\n",
    "        comp_str_data[f'ending{j}'].append(row['comp_struc_question_list'][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(comp_str_data)\n",
    "temp_train = temp.iloc[:int(len(temp)*0.8)]\n",
    "temp_valid = temp.iloc[int(len(temp)*0.8):int(len(temp)*0.9)]\n",
    "temp_test = temp.iloc[int(len(temp)*0.9):]\n",
    "temp_train.to_json('/mnt/hdd1/LaMDa/comp_str_data_train.json', lines=True, orient='records')\n",
    "temp_valid.to_json('/mnt/hdd1/LaMDa/comp_str_data_valid.json', lines=True, orient='records')\n",
    "temp_test.to_json('/mnt/hdd1/LaMDa/comp_str_data_test.json', lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "oxide_data = {f'ending{i}': [] for i in range(len(df['oxide_question_list'][0]))}\n",
    "oxide_data['sent1'] = []\n",
    "oxide_data['sent2'] = []\n",
    "oxide_data['label'] = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    oxide_data['sent1'].append('Which of the following statements is most correct regarding the given material?')\n",
    "    oxide_data['sent2'].append(str(row['atoms']))\n",
    "    oxide_data['label'].append(0)\n",
    "    for j in range(len(row['oxide_question_list'])):\n",
    "        oxide_data[f'ending{j}'].append(row['oxide_question_list'][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(oxide_data)\n",
    "temp_train = temp.iloc[:int(len(temp)*0.8)]\n",
    "temp_valid = temp.iloc[int(len(temp)*0.8):int(len(temp)*0.9)]\n",
    "temp_test = temp.iloc[int(len(temp)*0.9):]\n",
    "temp_train.to_json('/mnt/hdd1/LaMDa/oxide_data_train.json', lines=True, orient='records')\n",
    "temp_valid.to_json('/mnt/hdd1/LaMDa/oxide_data_valid.json', lines=True, orient='records')\n",
    "temp_test.to_json('/mnt/hdd1/LaMDa/oxide_data_test.json', lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clamp_p",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
